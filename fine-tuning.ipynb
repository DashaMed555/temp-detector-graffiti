{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03662fad-d225-4a34-ab6f-398b5c643b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/GroundingDINO/gd_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, GroundingDinoForObjectDetection, TrainingArguments, Trainer, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b5f56-4a06-4501-a1a7-ccffb2d4e3b2",
   "metadata": {},
   "source": [
    "Train:\n",
    "üéâ –í—Å–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!\n",
    "üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
    "   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 10687\n",
    "   –ê–Ω–Ω–æ—Ç–∞—Ü–∏–π: 4771\n",
    "   –ë–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: 5916\n",
    "   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 0.81:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d14bd7-35cc-4da9-8486-1b3728bf7609",
   "metadata": {},
   "source": [
    "val:\n",
    "üéâ –í—Å–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!\n",
    "üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
    "   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 2208\n",
    "   –ê–Ω–Ω–æ—Ç–∞—Ü–∏–π: 1106\n",
    "   –ë–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: 1102\n",
    "   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 1.00:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4af141c-b83d-4dee-8f1d-3cf18d6c6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "train_json_path = \"dataset/dataset/train_annotations.json\"\n",
    "val_json_path = \"dataset/dataset/train_annotations.json\"\n",
    "train_image_root = \"dataset/dataset/images_train\"\n",
    "val_image_root = \"dataset/dataset/images_train\"\n",
    "seed = 42\n",
    "output_dir = \"runs/gdino-trainer1\"\n",
    "labels_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be240110-1c03-422b-8e22-55229321e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, json_path, image_root, label2id):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        self.image_root = image_root\n",
    "        self.items = []\n",
    "\n",
    "        for entry in data:\n",
    "            w, h = entry[\"width\"], entry[\"height\"]\n",
    "            boxes = []\n",
    "            labels = []\n",
    "\n",
    "            for ann in entry[\"annotations\"]:\n",
    "                boxes.append([ann[\"cx\"], ann[\"cy\"], ann[\"w\"], ann[\"h\"]])\n",
    "                labels.append(label2id[str(ann[\"label_name\"])])\n",
    "\n",
    "            if len(boxes) > 0:\n",
    "                boxes_cxcywh = torch.tensor(boxes, dtype=torch.float32)\n",
    "                cx, cy, bw, bh = boxes_cxcywh.unbind(dim=-1)\n",
    "                boxes = torch.stack([cx, cy, bw, bh], dim=-1)\n",
    "                class_labels = torch.tensor(labels, dtype=torch.long)\n",
    "            else:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                class_labels = torch.zeros((0,), dtype=torch.long)\n",
    "\n",
    "            self.items.append({\n",
    "                \"image_path\": os.path.join(image_root, entry[\"image_name\"]),\n",
    "                \"size\": (h, w),\n",
    "                \"boxes\": boxes,\n",
    "                \"class_labels\": class_labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        it = self.items[idx]\n",
    "        image = Image.open(it[\"image_path\"]).convert(\"RGB\")\n",
    "        return {\"image\": image, **it}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    text_prompts = [text_prompt] * len(images)\n",
    "\n",
    "    enc = processor(\n",
    "        images=images,\n",
    "        text=text_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "    )\n",
    "\n",
    "    enc[\"model_inputs\"] = {k: v for k, v in enc.items()}\n",
    "\n",
    "    enc[\"labels\"] = [{\"class_labels\": b[\"class_labels\"], \"boxes\": b[\"boxes\"]} for b in batch]\n",
    "    enc[\"orig_sizes\"] = [b[\"size\"] for b in batch]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f971f2b6-13d2-41b4-8786-a61e7ae6601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model):\n",
    "    base_model = model.model\n",
    "\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if hasattr(base_model, \"encoder\"):\n",
    "        for layer in base_model.encoder.layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    if hasattr(base_model.decoder, \"reference_points_head\"):\n",
    "        for param in base_model.decoder.reference_points_head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(base_model.decoder, \"bbox_embed\"):\n",
    "        for param in base_model.decoder.bbox_embed.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # for name in [\"encoder_output_bbox_embed\", \"enc_output\", \"enc_output_norm\"]:\n",
    "    #     if hasattr(base_model, name):\n",
    "    #         for param in getattr(base_model, name).parameters():\n",
    "    #             param.requires_grad = True\n",
    "\n",
    "    total = sum(p.numel() for p in base_model.parameters())\n",
    "    trainable = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "    print(f\"–ó–∞–º–æ—Ä–æ–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable:,} / {total:,} \"\n",
    "          f\"({100 * trainable / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34916348-bd2c-4c5a-9733-fc80dcd516b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundingDINOTrainer(Trainer):\n",
    "    \n",
    "    def _build_model_inputs(self, batch, device):\n",
    "        if \"model_inputs\" in batch:\n",
    "            model_inputs = {k: v.to(device) for k, v in batch[\"model_inputs\"].items()}\n",
    "        else:\n",
    "            allowed = (\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\", \"pixel_mask\")\n",
    "            model_inputs = {k: v.to(device) for k, v in batch.items() if k in allowed and isinstance(v, torch.Tensor)}\n",
    "\n",
    "        if \"labels\" in batch:\n",
    "            labels_dev = []\n",
    "            for item in batch[\"labels\"]:\n",
    "                cls = item[\"class_labels\"]\n",
    "                boxes = item[\"boxes\"]\n",
    "                if not isinstance(cls, torch.Tensor):\n",
    "                    cls = torch.tensor(cls, dtype=torch.long)\n",
    "                if not isinstance(boxes, torch.Tensor):\n",
    "                    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "                labels_dev.append({\n",
    "                    \"class_labels\": cls.to(device),\n",
    "                    \"boxes\": boxes.to(device),\n",
    "                })\n",
    "            model_inputs[\"labels\"] = labels_dev\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        device = model.device\n",
    "        model_inputs = self._build_model_inputs(inputs, device)\n",
    "        outputs = model(**model_inputs)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "        model.eval()\n",
    "        device = model.device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        if \"model_inputs\" in inputs:\n",
    "            input_ids = inputs[\"model_inputs\"][\"input_ids\"]\n",
    "        else:\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        target_sizes = inputs.get(\"orig_sizes\", None)\n",
    "\n",
    "        results = processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            input_ids=input_ids,\n",
    "            box_threshold=0.4,\n",
    "            text_threshold=0.4,\n",
    "            target_sizes=target_sizes\n",
    "        )\n",
    "\n",
    "        preds = [torch.tensor(r[\"boxes\"], dtype=torch.float32, device=device) for r in results]\n",
    "        \n",
    "        W = target_sizes[0][1]\n",
    "        H = target_sizes[0][0]\n",
    "        labels = [] \n",
    "        for b in inputs[\"labels\"]: \n",
    "            boxes = b[\"boxes\"]\n",
    "            if not isinstance(boxes, torch.Tensor):\n",
    "                boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            cx, cy, bw, bh = boxes.unbind(-1)\n",
    "            gt_boxes = torch.stack([\n",
    "                (cx - bw/2) * W,   # x1\n",
    "                (cy - bh/2) * H,   # y1\n",
    "                (cx + bw/2) * W,   # x2\n",
    "                (cy + bh/2) * H    # y2\n",
    "            ], dim=-1)\n",
    "            labels.append(gt_boxes.to(device))\n",
    "        count_boxes = torch.tensor([preds[0].size()[0], labels[0].size()[0]], dtype=torch.float32, device=device)\n",
    "        return (loss, count_boxes, (preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc37870-c023-4f1b-8a1b-d41eb260e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred_boxes, target_boxes):\n",
    "    area1 = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    area2 = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
    "\n",
    "    lt = torch.max(pred_boxes[:, None, :2], target_boxes[:, :2])  # (N_pred, N_gt, 2)\n",
    "    rb = torch.min(pred_boxes[:, None, 2:], target_boxes[:, 2:])  # (N_pred, N_gt, 2)\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # (N_pred, N_gt, 2)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # (N_pred, N_gt)\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def match_predictions_to_targets(pred_boxes, target_boxes, iou_threshold=0.5):\n",
    "    ious = compute_iou(pred_boxes, target_boxes)\n",
    "    matched_gt = set()\n",
    "    correct = 0\n",
    "\n",
    "    for pred_idx in range(ious.shape[0]):\n",
    "        iou_values = ious[pred_idx]\n",
    "        best_gt_idx = iou_values.argmax().item()\n",
    "        best_iou = iou_values[best_gt_idx].item()\n",
    "\n",
    "        if best_iou >= iou_threshold and best_gt_idx not in matched_gt:\n",
    "            matched_gt.add(best_gt_idx)\n",
    "            correct += 1\n",
    "\n",
    "    return correct\n",
    "\n",
    "def compute_metrics(eval_preds, device=None, iou_threshold=0.5):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    count_boxes, (preds_batch, labels_batch) = eval_preds\n",
    "    count_boxes = [(int(count_boxes[i].item()), int(count_boxes[i + 1].item())) for i in range(0, len(count_boxes), 2)]\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_pred = 0\n",
    "    total_true = 0\n",
    "    fp_count = 0\n",
    "\n",
    "    curr_pred = 0\n",
    "    curr_gt = 0\n",
    "    for count_pred, count_gt in count_boxes:\n",
    "        total_pred += count_pred\n",
    "        total_true += count_gt\n",
    "        \n",
    "        if count_pred > 0 and count_gt > 0:\n",
    "            pred = preds_batch[0][curr_pred:curr_pred + count_pred]\n",
    "            gt = labels_batch[0][curr_gt:curr_gt + count_gt]\n",
    "            \n",
    "            if isinstance(pred, np.ndarray):\n",
    "                pred = torch.tensor(pred, dtype=torch.float32)\n",
    "            if isinstance(gt, np.ndarray):\n",
    "                gt = torch.tensor(gt, dtype=torch.float32)\n",
    "\n",
    "            correct = match_predictions_to_targets(pred, gt, iou_threshold=0.5)\n",
    "            total_correct += correct\n",
    "            if count_pred - correct:\n",
    "                fp_count += 1\n",
    "        else:\n",
    "            if count_pred > 0 and count_gt == 0:\n",
    "                fp_count += count_pred\n",
    "            \n",
    "        curr_pred += count_pred\n",
    "        curr_gt += count_gt\n",
    "        \n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0\n",
    "    recall = total_correct / total_true if total_true > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    false_positive_percentage = fp_count / len(count_boxes)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"false positive percentage\": false_positive_percentage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b805a475-fd98-4cd2-adcb-5b8410749f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßä –ó–∞–º–æ—Ä–æ–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 22,235,396 / 232,313,216 (9.57%)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "set_seed(seed)\n",
    "\n",
    "with open(train_json_path, \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "labels = labels_list if labels_list else sorted({ann[\"label_name\"] for d in train_data for ann in d[\"annotations\"]})\n",
    "label2id = {c: i for i, c in enumerate(labels)}\n",
    "id2label = {i: c for c, i in label2id.items()}\n",
    "\n",
    "train_ds = JsonDataset(train_json_path, train_image_root, label2id)\n",
    "val_ds = JsonDataset(val_json_path, val_image_root, label2id)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "freeze_layers(model)\n",
    "text_prompt = \" . \".join(labels) + \" .\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=128,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=1e-5,\n",
    "    eval_strategy='epoch',\n",
    "    eval_on_start=True,\n",
    "    remove_unused_columns=False,\n",
    "    weight_decay=3e-6,\n",
    "    adam_beta2=0.999,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"best\",\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")\n",
    "\n",
    "trainer = GroundingDINOTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71dcb52-02a6-4509-b06e-bd533db0cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/GroundingDINO/gd_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipykernel_3987636/3061116709.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds = [torch.tensor(r[\"boxes\"], dtype=torch.float32, device=device) for r in results]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1336' max='1336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1336/1336 1:05:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 394244.9375,\n",
       " 'eval_model_preparation_time': 0.0104,\n",
       " 'eval_precision': 0.891970802919708,\n",
       " 'eval_recall': 0.9934959349593496,\n",
       " 'eval_f1': 0.9399999950144972,\n",
       " 'eval_false positive percentage': 0.05538922155688623,\n",
       " 'eval_runtime': 3933.6517,\n",
       " 'eval_samples_per_second': 2.717,\n",
       " 'eval_steps_per_second': 0.34}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = GroundingDinoForObjectDetection.from_pretrained('checkpoint-334', id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "trainer_1 = GroundingDINOTrainer(\n",
    "    model=model_2,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer_1.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adb621-cccc-4ac7-9d7e-d382d2e612a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd_venv",
   "language": "python",
   "name": "gd_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
