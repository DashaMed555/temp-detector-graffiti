output_dir: "runs/fine-tuning"
seed: 42
per_device_train_batch_size: 2
per_device_eval_batch_size: 8
eval_accumulation_steps: 128
gradient_accumulation_steps: 32
num_train_epochs: 5
learning_rate: 1e-5
eval_strategy: "epoch"
eval_on_start: True
remove_unused_columns: False
weight_decay: 3e-6
adam_beta2: 0.999
optim: "adamw_torch"
save_strategy: "best"
load_best_model_at_end: True
bf16: True
dataloader_pin_memory: False
report_to: "tensorboard"
logging_strategy: "epoch"
metric_for_best_model: "f1"
greater_is_better: True
lr_scheduler_type: "cosine"
